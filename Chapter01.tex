\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{optidef}

\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\usepackage{graphicx}
\graphicspath{ {images} }

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\F}{\mathcal{F}}

	
\newcommand{\indep}{\perp \!\!\! \perp}

\newcommand{\prob}{\boldsymbol{P}}
\newcommand{\E}{\boldsymbol{E}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\cov}{\boldsymbol{Cov}}
\newcommand{\sd}{\text{SD}}
\newcommand{\se}{\text{SE}}
\newcommand{\dom}{\text{dom}}
\newcommand{\quartile}{\text{quartile}}

\newcommand{\trace}{\text{trace}}

\newcommand{\bin}{\text{Bin}}
\newcommand{\pois}{\text{Pois}}
\newcommand{\geom}{\text{Geom}}

\author{Arthur Chen}
\title{PACQ Solutions}
\date{\today}

\begin{document}

\maketitle

\section*{1 Introduction}

\subsection*{Exercise 1.2}

Let $W$ be an $r \times s$ random matrix, and let $A$ and $C$ be $n \times r$ and $n \times s$ matrices of constants. Show that $\E(AW+C) = A\E(W) + C$. If $B$ is $s \times t$ constant matrix, show that $\E(AWB) = A\E(W)B$. If $s=1$, show that $\cov(AW+C) = A\cov(W)A'$.

For $\E(AW+C)$, by definition $(AW+C)_{ij} = \sum_{k=1}^r A_{ik}W_{kj} + C_{ij}$. Taking expectations, we have

\[
\E((AW+C)_{ij}) = \sum_{k=1}^r A_{ik}\E(W_{kj}) + C_{ij}
\]

and we see that this equals $A\E(W)+C$.

For $\E(AWB)$, $\E(AWB) = A\E(WB)$ by the above.

\subsection*{Exercise 1.11}

Prove that if $Y \sim N(\mu, V)$ and $VAVBV=0$, $VAVB\mu=0$, $VBVA\mu=0$, and the conditions from Theorem 1.3.6 hold for $Y'AY$ and $Y'BY$, then $Y'AY$ and $Y'BY$ are independent.

Let $V=QQ'$ and rewrite $Y=\mu + QZ$, where $Z\sim N(0,I)$. We now show that the following variables are independent:

\begin{equation}
\begin{bmatrix}
Q'AQZ \\
\mu'AQZ
\end{bmatrix}
\indep
\begin{bmatrix}
Q'BQZ \\
\mu'BQZ
\end{bmatrix}
\end{equation}

Since these variables are all normal, showing uncorrelatedness shows independence. We first show this for one of the terms.

\begin{align}
\label{Exercise1.11Covariance}
\cov(Q'AQZ, Q'BQZ) &= \E(Q'AQZZ'Q'B'Q) - \E(Q'AQZ)E(Z'Q'B'Q) \\
&= Q'AQQ'B'Q = Q'AVBQ
\end{align}
because $Z \sim N(0, I)$ and $QQ' = V$. We have from the same argument as the proof of Theorem 1.3.6 in the book that $Q = Q_1Q_2$, where $Q_1$ has orthonormal columns and $Q_2$ is nonsingular. Thus by the same argument,
\[
Q_2^{-1}Q_1'V = Q'
\]
Applying this result to Equation \ref{Exercise1.11Covariance}, we get that
\[
\cov(Q'AQZ, Q'BQZ)=Q_2^{-1}Q_1'VAVBV'Q_1Q_2'^{-1} = 0
\]
since by assumption $VAVABV=0$. Similar results hold for the other cross terms.

\end{document}