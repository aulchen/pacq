\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{optidef}

\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\usepackage{graphicx}
\graphicspath{ {images} }

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\F}{\mathcal{F}}

	
\newcommand{\indep}{\perp \!\!\! \perp}

\newcommand{\prob}{\boldsymbol{P}}
\newcommand{\E}{\boldsymbol{E}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\cov}{\boldsymbol{Cov}}
\newcommand{\tr}{\boldsymbol{tr}}
\newcommand{\sd}{\text{SD}}
\newcommand{\se}{\text{SE}}
\newcommand{\dom}{\text{dom}}
\newcommand{\quartile}{\text{quartile}}

\newcommand{\trace}{\text{trace}}

\newcommand{\bin}{\text{Bin}}
\newcommand{\pois}{\text{Pois}}
\newcommand{\geom}{\text{Geom}}

\author{Arthur Chen}
\title{Chapter 1 Solutions}
\date{\today}

\begin{document}

\maketitle

\section*{Exercise 1.2}

Let $W$ be an $r \times s$ random matrix, and let $A$ and $C$ be $n \times r$ and $n \times s$ matrices of constants. Show that $\E(AW+C) = A\E(W) + C$. If $B$ is $s \times t$ constant matrix, show that $\E(AWB) = A\E(W)B$. If $s=1$, show that $\cov(AW+C) = A\cov(W)A'$.

For $\E(AW+C)$, by definition $(AW+C)_{ij} = \sum_{k=1}^r A_{ik}W_{kj} + C_{ij}$. Taking expectations, we have

\[
\E((AW+C)_{ij}) = \sum_{k=1}^r A_{ik}\E(W_{kj}) + C_{ij}
\]

and we see that this equals $A\E(W)+C$.

For $\E(AWB)$, $\E(AWB) = A\E(WB)$ by the above.

\section*{Exercise 1.3}

Show that $\cov(Y)$ is nonnegative definite for any random vector $Y$.

Let $z$ be an arbitrary fixed vector with the same length as $Y$. Then

\begin{align*}
z'\cov(Y)z &= z'\E[(Y-\mu)(Y-\mu)']z \\
&= \E[z'(Y-\mu)(Y-\mu)'z] \\
&= \E[||z'(Y-\mu)'||^2]
\end{align*}

and since $||z'(Y-\mu)'||^2 \geq 0$, its expectation is also $\geq 0$.

\section*{Exercise 1.4}

Let $M$ be the ppo onto $C(X)$. Show that $(I-M)$ is the ppo onto $C(X)^\perp$.

$I-M$ is symmetric and $(I-M)(I-M) = I-2M+M = I-M$ idempotent and thus is a ppo. It projects onto $C(X)^\perp$ because for $x \in C(X)$, $(I-M)x = x-x=0$ and for $x \in C(X)^\perp$, $(I-M)x = x$. For the trace, $\tr(I-M+M) = n$ implies that $\tr(I-M)+\tr(M)=n$. Since $\tr(M)=r(X)$, we have that $\tr(I-M)=n-r(X)$.

\section*{Exercise 1.6}

For a linear model $Y=X\beta+e$, $\E(e)=0$, $\cov(E)=\sigma^2I$, the residuals are

\[
\hat{e} = Y-X\hat{\beta} = (I-M)Y
\]

where $M$ is the perpendicular projection operator onto $C(X)$. Find the following.

\subsection*{Part a}

\[
\E(\hat{e}) = (I-M)\E(Y) = (I-M)X\beta = 0
\]

because $X\beta \in C(X)$.

\subsection*{Part b}

\[
\cov(\hat{e}) = (I-M)\cov(Y)(I-M)' = \sigma^2(I-M)
\]

because $I-M$ being a ppo implies $(I-M)(I-M)' = (I-M)$.

\subsection*{Part c}

\[
\cov(\hat{e},MY) = \cov(\hat{e},Y)M' = \cov((I-M)Y,Y)M = \sigma^2(I-M)M = 0
\]

\subsection*{Part d}

\[
\E(\hat{e}'\hat{e}) = \E[Y'(I-M)Y] = \E(e'(I-M)e)
\]

where the second equality uses $(I-M)Y = (I-M)X\beta+e = (I-M)e$ and the symmetry of $I-M$. By the distribution of quadratic forms,

\[
\E(e'(I-M)e) = \tr((I-M)(\sigma^2I)) = \sigma^2(n-r)
\]

\subsection*{Part e}

To show that $\hat{e}'\hat{e} = Y'Y-Y'MY$, this immediately follows from $\hat{e}'\hat{e}=Y'(I-M)Y$ and distributing.

\subsection*{Part f}

Using that $MY=X\hat{\beta}$, rewrite c. and e.

Making the substitution, we get that $\cov(\hat{\beta}, \hat{e}) = 0$ and $\hat{e}'\hat{e} = Y'Y - \hat{\beta}'Y$.

\section*{Exercise 1.7}

Given that $Y \sim N(\mu, V)$ and $V$ is nonsingular, show that the density of $Y$ is

\[
f(y) = (2\pi)^{-n/2}[\det(V)]^{-1/2}\exp(-(y-\mu)'V^{-1}(y-\mu)/2)
\]

We can write $V$ as $V = AA'$, where $A$ is nonsingular. By the definition of normals, we can rewrite $Y$ as $Y = G(Z)= \mu + AZ$, where $Z$, where $Z$ is a standard normal. Thus $G^{-1}(Y) = A^{-1}(Y-\mu)$,  $dG^{-1}(Y) = A^{-1}$, and

\[
\det(dG^{-1}(Y)) = \det(A^{-1}) = \det(A)^{-1} = \det(V)^{-1/2}
\]

since $AA' = V$ implies $\det(A) = \det(V)^{1/2}$. For the density of $Z$,

\begin{align*}
f_Z(z) &= f_Z(G^{-1}(y)) \\
&= (2\pi)^{-n/2}\exp(
-(Y-\mu)'A'^{-1}A^{-1}(Y-\mu)/2
) \\
&= (2\pi)^{-n/2}\exp(
-(Y-\mu)'V^{-1}(Y-\mu)/2
)
\end{align*}

since $AA'=V$ implies $A'^{-1}A^{-1}=V^{-1}$. Thus the final density of $Y$ is

\begin{align*}
f_Y(y) &= f_Z(G^{-1}(y))|\det(dG^{-1}(Y))| \\
&= (2\pi)^{-n/2}\det(V)^{-1/2}\exp(
-(Y-\mu)'V^{-1}(Y-\mu)/2
)
\end{align*}

as desired.

\section*{Exercise 1.8}

Show that if $Y\sim N(\mu, V)$ and $B$ is a fixed $n \times r$ matrix, then $BY \sim N(B\mu, BVB')$.

Let $Z$ be an r-dimensional standard normal. Define $Y = AZ + \mu$ where $AA' = V$. Then $BY = BAZ + B\mu$ is a normally distributed random variable with distribution $N(B\mu, BAA'B') = N(B\mu, BVB')$.

\section*{Exercise 1.11}

Prove that if $Y \sim N(\mu, V)$ and $VAVBV=0$, $VAVB\mu=0$, $VBVA\mu=0$, and the conditions from Theorem 1.3.6 hold for $Y'AY$ and $Y'BY$, then $Y'AY$ and $Y'BY$ are independent.

Let $V=QQ'$ and rewrite $Y=\mu + QZ$, where $Z\sim N(0,I)$. We now show that the following variables are independent:

\begin{equation}
\label{Exercise1.11Independence}
\begin{bmatrix}
Q'AQZ \\
\mu'AQZ
\end{bmatrix}
\indep
\begin{bmatrix}
Q'BQZ \\
\mu'BQZ
\end{bmatrix}
\end{equation}

Since these variables are all normal, showing uncorrelatedness shows independence. We first show this for one of the terms.

\begin{align}
\label{Exercise1.11Covariance}
\cov(Q'AQZ, Q'BQZ) &= \E(Q'AQZZ'Q'B'Q) - \E(Q'AQZ)E(Z'Q'B'Q) \\
&= Q'AQQ'B'Q = Q'AVBQ
\end{align}
because $Z \sim N(0, I)$ and $QQ' = V$. We have from the same argument as the proof of Theorem 1.3.6 in the book that $Q = Q_1Q_2$, where $Q_1$ has orthonormal columns and $Q_2$ is nonsingular. Thus by the same argument,
\[
Q_2^{-1}Q_1'V = Q'
\]
Applying this result to Equation \ref{Exercise1.11Covariance}, we get that
\[
\cov(Q'AQZ, Q'BQZ)=Q_2^{-1}Q_1'VAVBV'Q_1Q_2'^{-1} = 0
\]
since by assumption $VAVABV=0$. Similar results hold for the other cross terms.

We then have by the definition of $Y=\mu+QZ$ that

\begin{align*}
Y'AY &= (\mu'+Z'Q')A(QZ+\mu) \\
&= Z'Q'AQZ + Z'Q'A\mu + \mu'AQZ + \mu'A\mu
\end{align*}

The second, third, and fourth terms are trivially functions of variables on the left side of Equation \ref{Exercise1.11Independence}. The first term is, too, because

\begin{align*}
Z'Q'AQZ &= Z'Q_2^{-1}Q_1'VAVQ_1Q_2'^{-1}Z \\
&= Z'Q_2^{-1}Q_1'VAVAVQ_1Q_2'^{-1}Z \\
&= Z'Q_2^{-1}Q_1'VAQQ'AVQ_1Q_2'^{-1}Z \\
&= Z'Q'VAQQ'AQZ \\
&= (Q'AQZ)'(Q'AQZ) \\
&= ||Q'AQZ||^2
\end{align*}

is a function of $Q'AQZ$, where the second line follows from the assumption that $VAVAV=VAV$ from Theorem 1.3.6 in the book. A similar argument holds for $Y'BY$. Thus $Y'AY$ is a function of variables on the left side of Equation \ref{Exercise1.11Independence}, and $Y'BY$ a function of the right side. Since the two sides of Equation \ref{Exercise1.11Independence} are independent, $Y'AY$ and $Y'BY$ are independent.

\section*{Exercise 1.5.1}

Let $Y = (y_1, y_2, y_3)'$ be a random vector. Suppose $E(Y) \in M$, where

\[
M = \{
(a, a-b, 2b)'|a, b \in \R
\}
\]

\subsection*{Part a}

Show that $M$ is a vector space.

Let $x = (a_1, a_1-b_1, 2b_1)$ and $y = (a_2, a_2-b_2, 2b_2)$ be in $M$. Then

\[
cx+dy = \begin{pmatrix}
ca_1+da_2 \\
(ca_1+da_2) - (cb_1+db_2) \\
2(cb_1+db_2)
\end{pmatrix}
\]

is in $M$.

\subsection*{Part b}

Find a basis for $M$.

\[
\left\{
\begin{pmatrix}
1\\1\\0
\end{pmatrix},
\begin{pmatrix}
0\\-1\\2
\end{pmatrix}
\right\}
\]

is a basis for $M$.

\subsection*{Part c}

Find a linear model for the problem.

A linear model is

\[
X = \begin{pmatrix}
1 & 0 \\
1 & -1 \\
0 & 2
\end{pmatrix}
\]

such that $Y = X\beta + e$, $\E(e)=0$.

\subsection*{Part d}

Find two vectors $r$ and $s$ such that $\E(r'Y) = r'X\beta = \beta_1$ and $\E(s'Y) =\beta_2$. Find another vector $t \neq r$ such that $\E(t'Y)=\beta_1$.

Let $r = (1, 0, 0)'$, $s = (0, 0, 1/2)'$, and $t=(0, 1, -1/2)$.

\section*{Exercise 1.5.2}

Let

\[
Y \sim N\left(
\begin{pmatrix}
5 \\ 6 \\ 7
\end{pmatrix},
\begin{pmatrix}
2 & 0 & 1 \\
0 & 3 & 2 \\
1 & 2 & 4
\end{pmatrix}
\right)
\]

Find

\subsection*{Part a}

the marginal distribution of $y_1$.

$y_1 = AY$, where $A=(1, 0, 0)$. By Exercise 1.8, we have

\[
AY \sim N(A\mu, AVA') = N(5, 2)
\]

\subsection*{Part b}

the joint distribution of $y_1$ and $y_2$.

Letting $B=(1, 1, 0)$, we have

\[
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}
= BY \sim N\left(
\begin{pmatrix}
5 \\ 6
\end{pmatrix},
\begin{pmatrix}
2 & 0\\
0 & 3
\end{pmatrix}
\right)
\]

\subsection*{Part c}

the conditional distribution of $y_3$ given $y_1=u_1$ and $y_2=u_2$.

By the formulas for conditional distributions of normals, $y_3|y_1, y_2$ is normal with mean and covariance

\begin{gather*}
\mu_{3|1, 2} = 7 + (1, 2)\begin{pmatrix}
2 & 0 \\
0 & 3
\end{pmatrix}^{-1}
\begin{pmatrix}
u_1-5 \\ u_2-6
\end{pmatrix}
= 7 + \frac{1}{2}(u_1-5) + \frac{2}{3}(u_2-6) \\
\Sigma_{3|1,2} = 4-
\begin{pmatrix}
1 & 2
\end{pmatrix}
\begin{pmatrix}
2 & 0\\
0 & 3
\end{pmatrix}^{-1}
\begin{pmatrix}
1 \\ 2
\end{pmatrix} = 4-\frac{11}{6} = \frac{13}{6}
\end{gather*}

\subsection*{Part d}

the conditional distribution of $y_3$ given $y_1=u_1$.

From similar calculations to Part c we have that $y_3|y_1=y_1$ is normal with mean and covariance

\begin{gather*}
\mu_{3|1} = 7 + 1(2)^{-1}(u_1-5) = 7 + \frac{1}{2}(u_1-5) \\
\Sigma_{3|1} = 4-1(2)^{-1}1 = \frac{7}{2}
\end{gather*}

\subsection*{Part e}

the conditional distribution of $y_1$ and $y_2$ given $y_3 = u_3$.

$y_1, y_2|y_3$ is normal with mean and covariance

\begin{gather*}
\mu_{1,2|3} = \begin{pmatrix}
5 \\ 6
\end{pmatrix} +
\begin{pmatrix}
1 \\ 2
\end{pmatrix}
(4)^{-1}(u_3-7)
= \begin{pmatrix}5 \\ 6\end{pmatrix}
+\begin{pmatrix}
1/4 \\ 1/2
\end{pmatrix}(u_3-7)\\
\Sigma_{1,2|3} = \begin{pmatrix}
2&0\\
0&3
\end{pmatrix} -
\frac{1}{4}
\begin{pmatrix}
1 \\ 2
\end{pmatrix}
\begin{pmatrix}
1 & 2
\end{pmatrix} =
\begin{pmatrix}
7/4 & -1/2 \\
-1/2 & 2
\end{pmatrix}
\end{gather*}

\end{document}