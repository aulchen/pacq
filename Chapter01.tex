\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{optidef}

\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\usepackage{graphicx}
\graphicspath{ {images} }

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\F}{\mathcal{F}}

	
\newcommand{\indep}{\perp \!\!\! \perp}

\newcommand{\prob}{\boldsymbol{P}}
\newcommand{\E}{\boldsymbol{E}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\cov}{\boldsymbol{Cov}}
\newcommand{\tr}{\boldsymbol{tr}}
\newcommand{\sd}{\text{SD}}
\newcommand{\se}{\text{SE}}
\newcommand{\dom}{\text{dom}}
\newcommand{\quartile}{\text{quartile}}

\newcommand{\trace}{\text{trace}}

\newcommand{\bin}{\text{Bin}}
\newcommand{\pois}{\text{Pois}}
\newcommand{\geom}{\text{Geom}}

\author{Arthur Chen}
\title{Chapter 1 Solutions}
\date{\today}

\begin{document}

\maketitle

\section*{Exercise 1.2}

Let $W$ be an $r \times s$ random matrix, and let $A$ and $C$ be $n \times r$ and $n \times s$ matrices of constants. Show that $\E(AW+C) = A\E(W) + C$. If $B$ is $s \times t$ constant matrix, show that $\E(AWB) = A\E(W)B$. If $s=1$, show that $\cov(AW+C) = A\cov(W)A'$.

For $\E(AW+C)$, by definition $(AW+C)_{ij} = \sum_{k=1}^r A_{ik}W_{kj} + C_{ij}$. Taking expectations, we have

\[
\E((AW+C)_{ij}) = \sum_{k=1}^r A_{ik}\E(W_{kj}) + C_{ij}
\]

and we see that this equals $A\E(W)+C$.

For $\E(AWB)$, $\E(AWB) = A\E(WB)$ by the above.

\section*{Exercise 1.3}

Show that $\cov(Y)$ is nonnegative definite for any random vector $Y$.

Let $z$ be an arbitrary fixed vector with the same length as $Y$. Then

\begin{align*}
z'\cov(Y)z &= z'\E[(Y-\mu)(Y-\mu)']z \\
&= \E[z'(Y-\mu)(Y-\mu)'z] \\
&= \E[||z'(Y-\mu)'||^2]
\end{align*}

and since $||z'(Y-\mu)'||^2 \geq 0$, its expectation is also $\geq 0$.

\section*{Exercise 1.4}

Let $M$ be the ppo onto $C(X)$. Show that $(I-M)$ is the ppo onto $C(X)^\perp$.

$I-M$ is symmetric and $(I-M)(I-M) = I-2M+M = I-M$ idempotent and thus is a ppo. It projects onto $C(X)^\perp$ because for $x \in C(X)$, $(I-M)x = x-x=0$ and for $x \in C(X)^\perp$, $(I-M)x = x$. For the trace, $\tr(I-M+M) = n$ implies that $\tr(I-M)+\tr(M)=n$. Since $\tr(M)=r(X)$, we have that $\tr(I-M)=n-r(X)$.

\section*{Exercise 1.6}

For a linear model $Y=X\beta+e$, $\E(e)=0$, $\cov(E)=\sigma^2I$, the residuals are

\[
\hat{e} = Y-X\hat{\beta} = (I-M)Y
\]

where $M$ is the perpendicular projection operator onto $C(X)$. Find the following.

\subsection*{Part a}

\[
\E(\hat{e}) = (I-M)\E(Y) = (I-M)X\beta = 0
\]

because $X\beta \in C(X)$.

\subsection*{Part b}

\[
\cov(\hat{e}) = (I-M)\cov(Y)(I-M)' = \sigma^2(I-M)
\]

because $I-M$ being a ppo implies $(I-M)(I-M)' = (I-M)$.

\subsection*{Part c}

\[
\cov(\hat{e},MY) = \cov(\hat{e},Y)M' = \cov((I-M)Y,Y)M = \sigma^2(I-M)M = 0
\]

\subsection*{Part d}

\[
\E(\hat{e}'\hat{e}) = \E[Y'(I-M)Y] = \E(e'(I-M)e)
\]

where the second equality uses $(I-M)Y = (I-M)X\beta+e = (I-M)e$ and the symmetry of $I-M$. By the distribution of quadratic forms,

\[
\E(e'(I-M)e) = \tr((I-M)(\sigma^2I)) = \sigma^2(n-r)
\]

\subsection*{Part e}

To show that $\hat{e}'\hat{e} = Y'Y-Y'MY$, this immediately follows from $\hat{e}'\hat{e}=Y'(I-M)Y$ and distributing.

\subsection*{Part f}

Using that $MY=X\hat{\beta}$, rewrite c. and e.

Making the substitution, we get that $\cov(\hat{\beta}, \hat{e}) = 0$ and $\hat{e}'\hat{e} = Y'Y - \hat{beta}'Y$.

\section*{Exercise 1.11}

Prove that if $Y \sim N(\mu, V)$ and $VAVBV=0$, $VAVB\mu=0$, $VBVA\mu=0$, and the conditions from Theorem 1.3.6 hold for $Y'AY$ and $Y'BY$, then $Y'AY$ and $Y'BY$ are independent.

Let $V=QQ'$ and rewrite $Y=\mu + QZ$, where $Z\sim N(0,I)$. We now show that the following variables are independent:

\begin{equation}
\label{Exercise1.11Independence}
\begin{bmatrix}
Q'AQZ \\
\mu'AQZ
\end{bmatrix}
\indep
\begin{bmatrix}
Q'BQZ \\
\mu'BQZ
\end{bmatrix}
\end{equation}

Since these variables are all normal, showing uncorrelatedness shows independence. We first show this for one of the terms.

\begin{align}
\label{Exercise1.11Covariance}
\cov(Q'AQZ, Q'BQZ) &= \E(Q'AQZZ'Q'B'Q) - \E(Q'AQZ)E(Z'Q'B'Q) \\
&= Q'AQQ'B'Q = Q'AVBQ
\end{align}
because $Z \sim N(0, I)$ and $QQ' = V$. We have from the same argument as the proof of Theorem 1.3.6 in the book that $Q = Q_1Q_2$, where $Q_1$ has orthonormal columns and $Q_2$ is nonsingular. Thus by the same argument,
\[
Q_2^{-1}Q_1'V = Q'
\]
Applying this result to Equation \ref{Exercise1.11Covariance}, we get that
\[
\cov(Q'AQZ, Q'BQZ)=Q_2^{-1}Q_1'VAVBV'Q_1Q_2'^{-1} = 0
\]
since by assumption $VAVABV=0$. Similar results hold for the other cross terms.

We then have by the definition of $Y=\mu+QZ$ that

\begin{align*}
Y'AY &= (\mu'+Z'Q')A(QZ+\mu) \\
&= Z'Q'AQZ + Z'Q'A\mu + \mu'AQZ + \mu'A\mu
\end{align*}

The second, third, and fourth terms are trivially functions of variables on the left side of Equation \ref{Exercise1.11Independence}. The first term is, too, because

\begin{align*}
Z'Q'AQZ &= Z'Q_2^{-1}Q_1'VAVQ_1Q_2'^{-1}Z \\
&= Z'Q_2^{-1}Q_1'VAVAVQ_1Q_2'^{-1}Z \\
&= Z'Q_2^{-1}Q_1'VAQQ'AVQ_1Q_2'^{-1}Z \\
&= Z'Q'VAQQ'AQZ \\
&= (Q'AQZ)'(Q'AQZ) \\
&= ||Q'AQZ||^2
\end{align*}

is a function of $Q'AQZ$, where the second line follows from the assumption that $VAVAV=VAV$ from Theorem 1.3.6 in the book. A similar argument holds for $Y'BY$. Thus $Y'AY$ is a function of variables on the left side of Equation \ref{Exercise1.11Independence}, and $Y'BY$ a function of the right side. Since the two sides of Equation \ref{Exercise1.11Independence} are independent, $Y'AY$ and $Y'BY$ are independent.

\end{document}